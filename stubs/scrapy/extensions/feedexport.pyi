from scrapy import signals as signals
from scrapy.exceptions import NotConfigured as NotConfigured, ScrapyDeprecationWarning as ScrapyDeprecationWarning
from scrapy.utils.boto import is_botocore_available as is_botocore_available
from scrapy.utils.conf import feed_complete_default_values_from_settings as feed_complete_default_values_from_settings
from scrapy.utils.ftp import ftp_store_file as ftp_store_file
from scrapy.utils.log import failure_to_exc_info as failure_to_exc_info
from scrapy.utils.misc import create_instance as create_instance, load_object as load_object
from scrapy.utils.python import get_func_args as get_func_args, without_none_values as without_none_values
from typing import Any, Optional
from zope.interface import Interface

logger: Any

def build_storage(builder: Any, uri: Any, *args: Any, feed_options: Optional[Any] = ..., preargs: Any = ..., **kwargs: Any): ...

class IFeedStorage(Interface):
    def __init__(uri: Any, *, feed_options: Optional[Any] = ...) -> None: ...
    def open(spider: Any) -> None: ...
    def store(file: Any) -> None: ...

class BlockingFeedStorage:
    def open(self, spider: Any): ...
    def store(self, file: Any): ...

class StdoutFeedStorage:
    def __init__(self, uri: Any, _stdout: Optional[Any] = ..., *, feed_options: Optional[Any] = ...) -> None: ...
    def open(self, spider: Any): ...
    def store(self, file: Any) -> None: ...

class FileFeedStorage:
    path: Any = ...
    write_mode: Any = ...
    def __init__(self, uri: Any, *, feed_options: Optional[Any] = ...) -> None: ...
    def open(self, spider: Any): ...
    def store(self, file: Any) -> None: ...

class S3FeedStorage(BlockingFeedStorage):
    bucketname: Any = ...
    access_key: Any = ...
    secret_key: Any = ...
    keyname: Any = ...
    acl: Any = ...
    s3_client: Any = ...
    def __init__(self, uri: Any, access_key: Optional[Any] = ..., secret_key: Optional[Any] = ..., acl: Optional[Any] = ..., *, feed_options: Optional[Any] = ...) -> None: ...
    @classmethod
    def from_crawler(cls, crawler: Any, uri: Any, *, feed_options: Optional[Any] = ...): ...

class GCSFeedStorage(BlockingFeedStorage):
    project_id: Any = ...
    acl: Any = ...
    bucket_name: Any = ...
    blob_name: Any = ...
    def __init__(self, uri: Any, project_id: Any, acl: Any) -> None: ...
    @classmethod
    def from_crawler(cls, crawler: Any, uri: Any): ...

class FTPFeedStorage(BlockingFeedStorage):
    host: Any = ...
    port: Any = ...
    username: Any = ...
    password: Any = ...
    path: Any = ...
    use_active_mode: Any = ...
    overwrite: Any = ...
    def __init__(self, uri: Any, use_active_mode: bool = ..., *, feed_options: Optional[Any] = ...) -> None: ...
    @classmethod
    def from_crawler(cls, crawler: Any, uri: Any, *, feed_options: Optional[Any] = ...): ...

class _FeedSlot:
    file: Any = ...
    exporter: Any = ...
    storage: Any = ...
    batch_id: Any = ...
    format: Any = ...
    store_empty: Any = ...
    uri_template: Any = ...
    uri: Any = ...
    itemcount: int = ...
    def __init__(self, file: Any, exporter: Any, storage: Any, uri: Any, format: Any, store_empty: Any, batch_id: Any, uri_template: Any) -> None: ...
    def start_exporting(self) -> None: ...
    def finish_exporting(self) -> None: ...

class FeedExporter:
    @classmethod
    def from_crawler(cls, crawler: Any): ...
    crawler: Any = ...
    settings: Any = ...
    feeds: Any = ...
    slots: Any = ...
    storages: Any = ...
    exporters: Any = ...
    def __init__(self, crawler: Any) -> None: ...
    def open_spider(self, spider: Any) -> None: ...
    def close_spider(self, spider: Any): ...
    def item_scraped(self, item: Any, spider: Any) -> None: ...
