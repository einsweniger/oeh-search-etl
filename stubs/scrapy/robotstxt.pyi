from abc import ABCMeta, abstractmethod
from scrapy.utils.python import to_unicode as to_unicode
from typing import Any

logger: Any

def decode_robotstxt(robotstxt_body: Any, spider: Any, to_native_str_type: bool = ...): ...

class RobotParser(metaclass=ABCMeta):
    @classmethod
    @abstractmethod
    def from_crawler(cls, crawler: Any, robotstxt_body: Any) -> Any: ...
    @abstractmethod
    def allowed(self, url: Any, user_agent: Any) -> Any: ...

class PythonRobotParser(RobotParser):
    spider: Any = ...
    rp: Any = ...
    def __init__(self, robotstxt_body: Any, spider: Any) -> None: ...
    @classmethod
    def from_crawler(cls, crawler: Any, robotstxt_body: Any): ...
    def allowed(self, url: Any, user_agent: Any): ...

class ReppyRobotParser(RobotParser):
    spider: Any = ...
    rp: Any = ...
    def __init__(self, robotstxt_body: Any, spider: Any) -> None: ...
    @classmethod
    def from_crawler(cls, crawler: Any, robotstxt_body: Any): ...
    def allowed(self, url: Any, user_agent: Any): ...

class RerpRobotParser(RobotParser):
    spider: Any = ...
    rp: Any = ...
    def __init__(self, robotstxt_body: Any, spider: Any) -> None: ...
    @classmethod
    def from_crawler(cls, crawler: Any, robotstxt_body: Any): ...
    def allowed(self, url: Any, user_agent: Any): ...

class ProtegoRobotParser(RobotParser):
    spider: Any = ...
    rp: Any = ...
    def __init__(self, robotstxt_body: Any, spider: Any) -> None: ...
    @classmethod
    def from_crawler(cls, crawler: Any, robotstxt_body: Any): ...
    def allowed(self, url: Any, user_agent: Any): ...
